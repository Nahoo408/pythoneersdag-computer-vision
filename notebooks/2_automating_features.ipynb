{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c64737-c4c3-4a33-a98a-ffeff8ed926b",
   "metadata": {},
   "source": [
    "# 2. Automating features\n",
    "\n",
    "In this notebook we will continue our search for the lost easter eggs. Using hand-crafted features can be very time-consuming, and it may yield poor results. Therefore, we will try and automate feature selection in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Add the project root to sys.path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "from utils.image_tools import read_image, show_img_inline, rgb_to_grayscale, draw_rectangle\n",
    "\n",
    "data_path = project_root / \"data\" / \"2_automating_features\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f859118-7f76-4f65-9075-bd10f927dd1b",
   "metadata": {},
   "source": [
    "## Template matching\n",
    "\n",
    "ðŸ•’ Estimated time: 25 minutes\n",
    "\n",
    "Before, we tried to create features by hand. It can be rather difficult to create features that are able to consistently detect objects. What if we didn't need to tweak the features every time to get an optimal detection? Eggs are rather simple objects, with relatively little variation. So why can't we just use a template, wouldn't that be the best feature-set there is?\n",
    "\n",
    "Template matching is what the name implies: a template is moved over the image, and the resulting likeness is used as a measure whether or not the object is present at that location.\n",
    "\n",
    "We have already created a template for you, so run the code below to get started. First, let's visualise the images, so we know what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb1db5-021d-4cc3-bc48-9560dc19c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = str(data_path / \"egg\" /\"image_01.jpg\")\n",
    "template_path = str(data_path / \"egg\" / \"egg_template.jpg\")\n",
    "\n",
    "rgb_image = read_image(image_path)\n",
    "grayscale_image = rgb_to_grayscale(rgb_image)\n",
    "template = rgb_to_grayscale(read_image(template_path))\n",
    "\n",
    "show_img_inline(rgb_image)\n",
    "show_img_inline(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40825db5-9389-481b-b8ab-db263fe2db40",
   "metadata": {},
   "source": [
    "Now we will use the `cv2.matchTemplate`-method to perform the template matching and visualise the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032ccc7-f9e3-435c-80f1-8822028da4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "likeness = cv2.matchTemplate(grayscale_image, template, cv2.TM_CCOEFF_NORMED) \n",
    "show_img_inline(likeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbfe7d5-8011-43f6-a0cb-6494b074e4b3",
   "metadata": {},
   "source": [
    "**Question:** What do you see?\n",
    "\n",
    "<details>\n",
    "<summary>answer</summary>\n",
    "The <code>cv2.matchTemplate</code>-method outputs the likeness between the template and that part of the image for every pixel. So the resulting output can be interpreted as a heatmap, with higher values indicating a higher likeness.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a18ba42-ec93-4381-b88a-74b9b1e3a8fe",
   "metadata": {},
   "source": [
    "Now we will apply the threshold and draw the resulting detections on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ec930-d9b8-4164-8d54-8e2e5c493491",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "drawable_image = rgb_image.copy()\n",
    "\n",
    "thresholded_likeness = np.where(likeness >= threshold)\n",
    "\n",
    "width, height = template.shape[::-1]\n",
    "for x_center, y_center in zip(*thresholded_likeness[::-1]):\n",
    "    draw_rectangle(drawable_image, (x_center, y_center, width, height))\n",
    "\n",
    "show_img_inline(drawable_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773e934-fa1a-4390-bb61-981eddb5975e",
   "metadata": {},
   "source": [
    "You may have noticed that there are quite a lot of detections, we can use the `cv2.minMacLoc`-method to just get the location of the highest (and lowest) likeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f23df-742b-4fd9-a248-e7dd87f7612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawable_image = rgb_image.copy()\n",
    "\n",
    "min_likeness, max_likeness, min_location, max_location = cv2.minMaxLoc(likeness)\n",
    "x_center, y_center = max_location\n",
    "\n",
    "width, height = template.shape[::-1]\n",
    "draw_rectangle(drawable_image, (x_center, y_center, width, height), colour=(0, 255, 0))\n",
    "\n",
    "show_img_inline(drawable_image)\n",
    "max_likeness  # displays the likeness score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0540ad5-bcfc-459a-a046-2edca521ef94",
   "metadata": {},
   "source": [
    "All right, we have a working detector using template matching. Now let us find all the eggs, for example in the next image (`image_02.jpg`)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2b538-653e-48dc-9b5d-cf528e75d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = str(data_path / \"egg\" / \"image_02.jpg\")\n",
    "template_path = str(data_path / \"egg\" / \"egg_template.jpg\")\n",
    "threshold = 0.99\n",
    "\n",
    "rgb_image = read_image(image_path)\n",
    "grayscale_image = rgb_to_grayscale(rgb_image)\n",
    "drawable_image = rgb_image.copy()\n",
    "template = rgb_to_grayscale(read_image(template_path))\n",
    "\n",
    "likeness = cv2.matchTemplate(grayscale_image, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "# Calculate and draw thresholded likeness in blue\n",
    "thresholded_likeness = np.where(likeness >= threshold)\n",
    "width, height = template.shape[::-1]\n",
    "for x_center, y_center in zip(*thresholded_likeness[::-1]):\n",
    "    draw_rectangle(drawable_image, (x_center, y_center, width, height))\n",
    "\n",
    "show_img_inline(drawable_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f5055-6838-4d3f-831a-9490480f0495",
   "metadata": {},
   "source": [
    "Erm, it seems that we did not detect any eggs.\n",
    "\n",
    "**Question:** What could be the cause of this?\n",
    "\n",
    "<details>\n",
    "<summary>answer</summary>\n",
    "The threshold is too high. Seeing as the template was taken from the previous image and not this one, a likeness of 0.99 may be too much.\n",
    "\n",
    "So, retry with different thresholds to get the detection to work again.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6288ed7-e5d6-4471-aecb-a23dc8b9f847",
   "metadata": {},
   "source": [
    "With some changes we still got a detection, but it seems the detection does not line up perfectly with the egg.\n",
    "\n",
    "**Question:** Why is that?\n",
    "\n",
    "<details>\n",
    "<summary>answer</summary>\n",
    "Template matching only finds the optimal location where the template has the highest likeness to the pixels in the image. The width and height of the detection are the width and height of the template.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bf9bf-938a-4c34-97c5-d5c8b437e62e",
   "metadata": {},
   "source": [
    "Now change the image to `image_03.jpg` and see how the different eggs are detected. Try to detect all eggs.\n",
    "\n",
    "**Question:** Why are some eggs detected more easily compared to others?\n",
    "\n",
    "hint: Take a look at the template itself.\n",
    "\n",
    "<details>\n",
    "<summary>answer</summary>\n",
    "Template matching simply tries to match a template to a part of the image. If the object is rotated or scaled, template matching can struggle. There are some methods to make template matching a bit more robust to this kind of variation, but it is a fundamental shortcoming of this method.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5e097-10b6-4349-bb67-e6cd06cd1792",
   "metadata": {},
   "source": [
    "Next, change the image to `image_04.jpg` and try to detect the egg.\n",
    "\n",
    "**Question:** Did you succeed in detecting (only) the egg, why is that?\n",
    "\n",
    "<details>\n",
    "<summary>answer</summary>\n",
    "Template matching requires the object in the image to be highly similar to the template. If it is not (because of lighting, perspective, size, shape, occlusion etc.) this method may not work at all.\n",
    "\n",
    "So, template matching is very easy (and very fast!), making it ideal in some situations. However, today we need our top-of-the-line detection methods, because we need to find all the easter eggs. So let's get a move on!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9b88e-11e8-48e8-ba14-693cfbfffb9f",
   "metadata": {},
   "source": [
    "Finally, try using `image_05.jpg` and see what happens.\n",
    "\n",
    "**Question:** What happened when we performed template matching with the non-egg image?\n",
    "\n",
    "<details>\n",
    "<summary>answer</summary>\n",
    "The golf ball was identified as an egg. We call this a \"false positive\", meaning something is detected even though it is not. We can also have \"false negatives\", meaning that we did not detect an object even though it does exist. These are the two ways in which detections fail. The cost of a false positive may be different from the cost of a false negative.\n",
    "\n",
    "For example, we may want to build a detector that finds all the eggs. In that case, we want to minimize the false negatives more than the false positives (finding a free golf ball is not bad either). \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea1bdf-1b45-4181-a465-34319e52be45",
   "metadata": {},
   "source": [
    "## Haar cascade classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a182e86-b930-46f6-997f-d0375152a740",
   "metadata": {},
   "source": [
    "ðŸ•’ Estimated time: 15 minutes\n",
    "\n",
    "We have concluded earlier that hand-crafing features is time consuming and difficult, and sadly template matching was no silver bullet either. Luckily, we can automatically determine which features are relevant if we want to detect a specific object.\n",
    "\n",
    "One method is called \"Haar cascade classifier\". This method takes a large number of features (e.g. 6000 unique features), and learns which features are relevant to detecting an object. Now all we need to do is apply all features to our image on a sliding window. This is rather time-consuming, so the Haar cascade classifier not only learns which features are important, but also their relative importance. The features are grouped into stages, sorted by their importance. The most important features are applied first, and if they determine no object is present the algorithm moves on to the next location. If the first features determine there may be an object however, the next stage is applied, and so on.\n",
    "Haar cascade classifiers are able to detect more complex objects compared to template matching. Cats are very diverse (colour, shape etc.), so using a template would not work.\n",
    "\n",
    "Let's see how easy it is to use such classifiers using OpenCV.\n",
    "\n",
    "We have already selected an image, so let's run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e9502-e9df-4337-8b48-a02ec8bff5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = str(data_path / \"cat\" / \"image_01.jpg\")\n",
    "\n",
    "rgb_image = read_image(image_path)\n",
    "grayscale_image = rgb_to_grayscale(rgb_image)\n",
    "\n",
    "show_img_inline(rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e326c8-946b-4ef5-bbe4-921deed6bc07",
   "metadata": {},
   "source": [
    "We are still building a detector to look for our easter eggs. But wait, what is that?! We have a competitor in our midst, oh no! Mr. Cat McCatface is also looking for the eggs, and he does not look friendly. It is best to stay out of his way, so let us try to detect him (and his kin) as well.\n",
    "\n",
    "OpenCV supplies pre-trained cascade classifiers, and luckily for us there is one for cat faces as well. Run the next cell to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b7542-6ded-4a32-abd0-20b2afb346eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_path = f\"{cv2.data.haarcascades}haarcascade_frontalcatface.xml\"\n",
    "# all avaliable classifiers: https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "\n",
    "cascade_classifier = cv2.CascadeClassifier(classifier_path)\n",
    "\n",
    "detections = cascade_classifier.detectMultiScale(grayscale_image)\n",
    "\n",
    "drawable_image = rgb_image.copy()\n",
    "for detection in detections:\n",
    "    draw_rectangle(drawable_image, detection)\n",
    "\n",
    "show_img_inline(drawable_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592f19e-f7d5-4813-9175-90da9c8cd62b",
   "metadata": {},
   "source": [
    "Luckily our classifier works well. What a relief! We do need to make sure it works robustly, so we must dig a little deeper before we can continue our search for the eggs.\n",
    "\n",
    "Try the classifier on the other images in the `/data/2_automating_features/cat`-directory and determine on which images the classifier performs well and where it struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abf349-e653-4e99-9e70-1d59423f3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = str(data_path / \"cat\" / \"image_01.jpg\")\n",
    "\n",
    "rgb_image = read_image(image_path)\n",
    "grayscale_image = rgb_to_grayscale(rgb_image)\n",
    "drawable_image = rgb_image.copy()\n",
    "\n",
    "cascade_classifier = cv2.CascadeClassifier(f\"{cv2.data.haarcascades}haarcascade_frontalcatface_extended.xml\")\n",
    "\n",
    "detections = cascade_classifier.detectMultiScale(grayscale_image)\n",
    "for detection in detections:\n",
    "    draw_rectangle(drawable_image, detection)\n",
    "\n",
    "show_img_inline(drawable_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2f52a-4c97-44d2-b7cd-2d144a77a016",
   "metadata": {},
   "source": [
    "hint: for some images, changing the parameters `minSize` and `maxSize` for the `detectMultiScale`-function leads to improved results. However, this introduces a new chore for us: parameter tuning. This is a concept in machine learning that refers to the setting of certain (model) parameters. Setting them right can be very difficult, and may lead to non-generalizable results. This means that the settings we found work very well for the data at hand, but if we get new data the settings may not work as well.\n",
    "\n",
    "**Question (to be discussed plenary)**: when does this cascading classifier work well, and where does it fall short?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1231b82-c71c-4e92-875c-8e3d3b3e37ef",
   "metadata": {},
   "source": [
    "**Note**: it is possible to train cascading classifiers. To do so, you need both positive and negative examples. For every positive example, you need to indicate the location of the object(s) of interest. This data can then be used to train a classifier that is tailor-made for your use-case. We opted not\n",
    "to do this due to the impracticality of OpenCVs cascading classifier training-implementation.\n",
    "If you are interested however, check out: **[training cascade classifier](https://docs.opencv.org/4.11.0/dc/d88/tutorial_traincascade.html)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
